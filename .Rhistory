"meanWordSyllables",
"Flesch",
"Flesch.Kincaid")
# loop across methods types
for (i in seq_along(methods)) {
# pass method to character string object
complex_method <- methods[[i]]
# estimate complexity, grouping by username
test <- corpus_speeches %>%
textstat_readability(complex_method) # specify method here as character object created above
testm <- as.matrix(test) #convert to a matrix
#rename column
#colnames(testm) <- "corr_may"
#create column variable from rownames
testdm <- tibble::rownames_to_column(testm, "username")
#record method in new column variable
testm$method <- complex_method
#record speaker
#bind all together
complexity_eu <- rbind(complexity_eu, testm)
}
# loop across methods types
for (i in seq_along(methods)) {
# pass method to character string object
complex_method <- methods[[i]]
# estimate complexity, grouping by username
test <- corpus_speeches %>%
textstat_readability(complex_method) # specify method here as character object created above
testm <- as.data.frame(test) #convert to a matrix
#rename column
#colnames(testm) <- "corr_may"
#create column variable from rownames
testdm <- tibble::rownames_to_column(testm, "speaker")
#record method in new column variable
testm$method <- complex_method
#record speaker
#bind all together
complexity_eu <- rbind(complexity_eu, testm)
}
# loop across methods types
for (i in seq_along(methods)) {
# pass method to character string object
complex_method <- methods[[i]]
# estimate complexity, grouping by username
test <- corpus_speeches %>%
textstat_readability(complex_method) # specify method here as character object created above
testm <- as.data.frame(test) #convert to a matrix
#bind all together
complexity_eu <- rbind(complexity_eu, testm)
}
# create empty dataframe
complexity_eu <- data.frame()
# loop across methods types
for (i in seq_along(methods)) {
# pass method to character string object
complex_method <- methods[[i]]
# estimate complexity, grouping by username
test <- corpus_speeches %>%
textstat_readability(complex_method) # specify method here as character object created above
testm <- as.data.frame(test) #convert to a matrix
#bind all together
complexity_eu <- rbind(complexity_eu, testm)
}
# loop across methods types
for (i in seq_along(methods)) {
# pass method to character string object
complex_method <- methods[[i]]
# estimate complexity, grouping by username
test <- corpus_speeches %>%
textstat_readability(complex_method) # specify method here as character object created above
testm <- as.data.frame(test) #convert to a matrix
testm$method <- complex_method
#bind all together
complexity_eu <- rbind(complexity_eu, testm)
}
# loop across methods types
for (i in seq_along(methods)) {
# pass method to character string object
complex_method <- methods[[i]]
# estimate complexity, grouping by username
test <- corpus_speeches %>%
textstat_readability(complex_method) # specify method here as character object created above
testm <- as.data.frame(test) #convert to a matrix
testm$method <- complex_method
#bind all together
complexity_eu <- complexity_eu %>%
full_join(testm)
}
complexity_eu
unique(complexity_eu$document)
library(questionr)
freq(complexity_eu$document)
# create empty dataframe
head(speeches)
speeches <- speeches %>%
mutate(meanSentenceLength = textstat_readability(text, measure = "meanSentenceLength"))
speeches
head(speeches)
View(speeches)
speeches  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/comparison-complexity/speeches.rds?raw=true")))
speeches$flesch.kincaid <- textstat_readability(speeches$text, measure = "Flesch.Kincaid")
# returned as quanteda data.frame with document-level information;
# need just the score:
speeches$flesch.kincaid <- speeches$flesch.kincaid$Flesch.Kincaid
# specify different complexity measures to explore
speeches <- speeches %>%
mutate(meansentencelength = textstat_readability(text, measure = "meanSentenceLength"),
meanwordsyllables = textstat_readability(text, measure = "meanSentenceLength"),
flesch = textstat_readability(text, measure = "Flesch"))
View(speeches)
names(speeches)
View(speeches)
# keep only the scores for the complexity measures (not the document identifier)
speeches <- speeches %>%
mutate(meansentencelength = meansentencelength$meanSentenceLength)
View(speeches)
speeches <- speeches %>%
mutate(meansentencelength = textstat_readability(text, measure = "meanSentenceLength"),
meanwordsyllables = textstat_readability(text, measure = "meanWordSyllables"),
flesch = textstat_readability(text, measure = "Flesch"))
names(speeches)
# keep only the scores for the complexity measures (not the document identifier)
speeches <- speeches %>%
mutate(meansentencelength = meansentencelength$meanSentenceLength,
meanwordsyllables = meanwordsyllables$meanWordSyllables,
flesch = flesch$Flesch)
View(speeches)
sum_corpus <- speeches %>%
group_by(speaker) %>%
summarise(
across(c(flesch.kincaid:flesh), ~mean(na.rm=TRUE), .names = "mean_{.col}"),
across(c(flesch.kincaid:flesh), ~sd(na.rm=TRUE), .names = "n_{.col}"),
across(c(flesch.kincaid:flesh), ~length(speaker), .names = "n_{.col}"))
#get mean and standard deviation for each, and N of speeches for each speaker
sum_corpus <- speeches %>%
group_by(speaker) %>%
summarise(
across(c(flesch.kincaid:flesch), ~mean(na.rm=TRUE), .names = "mean_{.col}"),
across(c(flesch.kincaid:flesch), ~sd(na.rm=TRUE), .names = "n_{.col}"),
across(c(flesch.kincaid:flesch), ~length(speaker), .names = "n_{.col}"))
sum_corpus <- speeches %>%
group_by(speaker) %>%
summarise(
across(c(flesch.kincaid, meansentencelength, meanwordsyllables, flesch), ~mean(na.rm=TRUE), .names = "mean_{.col}"))
#get mean and standard deviation for each, and N of speeches for each speaker
sum_corpus <- speeches %>%
group_by(speaker) %>%
summarise(
across(c(flesch.kincaid, meansentencelength, meanwordsyllables, flesch), ~mean(.x, na.rm=TRUE), .names = "mean_{.col}"))
sum_corpus
sum_corpus <- speeches %>%
group_by(speaker) %>%
summarise(
across(c(flesch.kincaid, meansentencelength, meanwordsyllables, flesch),
~mean(.x, na.rm=TRUE), .names = "mean_{.col}"),
across(c(flesch.kincaid, meansentencelength, meanwordsyllables, flesch),
~sd(.x, na.rm=TRUE), .names = "sd_{.col}"))
sum_corpus
#get mean and standard deviation for each, and N of speeches for each speaker
sum_corpus <- speeches %>%
group_by(speaker) %>%
summarise(
across(c(flesch.kincaid, meansentencelength, meanwordsyllables, flesch),
~mean(.x, na.rm=TRUE), .names = "mean_{.col}"),
across(c(flesch.kincaid, meansentencelength, meanwordsyllables, flesch),
~sd(.x, na.rm=TRUE), .names = "sd_{.col}"),
n=length(speaker))
sum_corpus
sum_corpus <- sum_corpus %>%
mutate(se_flesch.kincaid = sd_flesch.kincaid/sqrt(n))
sum_corpus
sum_corpus <- sum_corpus %>%
mutate(# standard errors
se_flesch.kincaid = sd_flesch.kincaid/sqrt(n),
se_meansentencelength = sd_meansentencelength/sqrt(n),
se_meanwordsyllables = sd_meanwordsyllables/sqrt(n),
se_flesch = sd_flesch/sqrt(n),
# lower brackets of confidence interval
min_flesch.kincaid = mean_flesch.kincaid - 1.96*se_flesch.kincaid,
min_meansentencelength = mean_meansentencelength - 1.96*se_meansentencelength,
min_meanwordsyllables = mean_meanwordsyllables - 1.96*se_meanwordsyllables,
min_flesch = mean_flesch - 1.96*se_flesch)
sum_corpus
sum_corpus <- sum_corpus %>%
mutate(# standard errors
se_flesch.kincaid = sd_flesch.kincaid/sqrt(n),
se_meansentencelength = sd_meansentencelength/sqrt(n),
se_meanwordsyllables = sd_meanwordsyllables/sqrt(n),
se_flesch = sd_flesch/sqrt(n),
# lower brackets of confidence intervals
min_flesch.kincaid = mean_flesch.kincaid - 1.96*se_flesch.kincaid,
min_meansentencelength = mean_meansentencelength - 1.96*se_meansentencelength,
min_meanwordsyllables = mean_meanwordsyllables - 1.96*se_meanwordsyllables,
min_flesch = mean_flesch - 1.96*se_flesch,
# upper brackets of confidence intervals
max_flesch.kincaid = mean_flesch.kincaid + 1.96*se_flesch.kincaid,
max_meansentencelength = mean_meansentencelength + 1.96*se_meansentencelength,
max_meanwordsyllables = mean_meanwordsyllables + 1.96*se_meanwordsyllables,
max_flesch = mean_flesch + 1.96*se_flesch)
names(sum_corpus)
sum_corpus_long <- sum_corpus %>%
pivot_longer(cols = !c(speaker, n),
names_to = c("measure", "method"),
names_pattern = "(.*)_(.*)",
values_to = "values")
library(tidyverse) # I add this as I need it for the exercise
sum_corpus_long <- sum_corpus %>%
pivot_longer(cols = !c(speaker, n),
names_to = c("measure", "method"),
names_pattern = "(.*)_(.*)",
values_to = "values")
sum_corpus_long
sum_corpus_long2 <- sum_corpus %>%
pivot_longer(cols = !c(speaker, n),
names_to = c("method"),
names_pattern = "_(.*)")
sum_corpus_long2
sum_corpus
sum_corpus_long
sum_corpus %>%
select(speaker, n, str_detect("mean")) %>%
pivot_longer(cols = !c(speaker, n),
names_to = c("method"),
names_pattern = "mean_(.*)",
values_to = "mean")
sum_corpus %>%
select(speaker, n, matches("mean")) %>%
pivot_longer(cols = !c(speaker, n),
names_to = c("method"),
names_pattern = "mean_(.*)",
values_to = "mean")
sum_corpus %>%
select(speaker, n, matches("mean"))
sum_corpus %>%
select(speaker, n, matches("mean_")) %>%
pivot_longer(cols = !c(speaker, n),
names_to = c("method"),
names_pattern = "mean_(.*)",
values_to = "mean")
sum_corpus_long_min <- sum_corpus %>%
select(speaker, n, matches("min_")) %>%
pivot_longer(cols = !c(speaker, n),
names_to = c("method"),
names_pattern = "min_(.*)",
values_to = "min")
sum_corpus_long_min
sum_corpus_long_max <- sum_corpus %>%
select(speaker, n, matches("max_")) %>%
pivot_longer(cols = !c(speaker, n),
names_to = c("method"),
names_pattern = "max_(.*)",
values_to = "max")
# merge all
corpus_long_max <- sum_corpus_long_mean %>%
full_join(sum_corpus_long_min, by=c(speaker, n, method))
sum_corpus_long_mean <- sum_corpus %>%
select(speaker, n, matches("mean_")) %>%
pivot_longer(cols = !c(speaker, n),
names_to = c("method"),
names_pattern = "mean_(.*)",
values_to = "mean")
# merge all
corpus_long_max <- sum_corpus_long_mean %>%
full_join(sum_corpus_long_min, by=c(speaker, n, method))
sum_corpus_long_mean
# pivot lower confidence interval brackets
sum_corpus_long_min <- sum_corpus %>%
select(speaker, n, matches("min_")) %>%
pivot_longer(cols = !c(speaker, n),
names_to = c("method"),
names_pattern = "min_(.*)",
values_to = "min")
# pivot upper confidence interval brackets
sum_corpus_long_max <- sum_corpus %>%
select(speaker, n, matches("max_")) %>%
pivot_longer(cols = !c(speaker, n),
names_to = c("method"),
names_pattern = "max_(.*)",
values_to = "max")
# merge all
corpus_long_max <- sum_corpus_long_mean %>%
full_join(sum_corpus_long_min, by=c("speaker", "n", "method"))
corpus_long_max
# merge all
sum_corpus_long <- sum_corpus_long_mean %>%
full_join(sum_corpus_long_min, by=c("speaker", "n", "method")) %>%
full_join(sum_corpus_long_max, by=c("speaker", "n", "method")) %>%
# merge all
sum_corpus_long <- sum_corpus_long_mean %>%
full_join(sum_corpus_long_min, by=c("speaker", "n", "method")) %>%
full_join(sum_corpus_long_max, by=c("speaker", "n", "method")) %>%
sum_corpus_long
sum_corpus_long
# pivot means
sum_corpus_long_mean <- sum_corpus %>%
select(speaker, n, matches("mean_")) %>%
pivot_longer(cols = !c(speaker, n),
names_to = c("method"),
names_pattern = "mean_(.*)",
values_to = "mean")
# pivot lower confidence interval brackets
sum_corpus_long_min <- sum_corpus %>%
select(speaker, n, matches("min_")) %>%
pivot_longer(cols = !c(speaker, n),
names_to = c("method"),
names_pattern = "min_(.*)",
values_to = "min")
# pivot upper confidence interval brackets
sum_corpus_long_max <- sum_corpus %>%
select(speaker, n, matches("max_")) %>%
pivot_longer(cols = !c(speaker, n),
names_to = c("method"),
names_pattern = "max_(.*)",
values_to = "max")
# merge all
sum_corpus_long <- sum_corpus_long_mean %>%
full_join(sum_corpus_long_min, by=c("speaker", "n", "method")) %>%
full_join(sum_corpus_long_max, by=c("speaker", "n", "method")) %>%
# merge all
sum_corpus_long <- sum_corpus_long_mean %>%
full_join(sum_corpus_long_min, by=c("speaker", "n", "method")) %>%
full_join(sum_corpus_long_max, by=c("speaker", "n", "method"))
sum_corpus_long
sum_corpus_long %>%
ggplot(aes(x=speaker, y=values, colour=method)) +
geom_bar(stat="identity") +
geom_errorbar(ymin=sum_corpus_long$min,ymax=sum_corpus_long$max, width=.2) +
coord_flip() +
xlab("") +
ylab("Mean Complexity") +
theme_minimal() +
ylim(c(0,20))
sum_corpus_long %>%
ggplot(aes(x=speaker, y=mean, colour=method)) +
geom_bar(stat="identity") +
geom_errorbar(ymin=sum_corpus_long$min,ymax=sum_corpus_long$max, width=.2) +
coord_flip() +
xlab("") +
ylab("Mean Complexity") +
theme_minimal() +
ylim(c(0,20))
sum_corpus_long %>%
ggplot(aes(x=speaker, y=mean, group=method, colour=method)) +
geom_bar(stat="identity") +
geom_errorbar(ymin=sum_corpus_long$min,ymax=sum_corpus_long$max, width=.2) +
coord_flip() +
xlab("") +
ylab("Mean Complexity") +
theme_minimal() +
ylim(c(0,20))
sum_corpus_long %>%
ggplot(aes(x=method, y=mean, colour=method)) +
geom_column() +
geom_errorbar(ymin=sum_corpus_long$min,ymax=sum_corpus_long$max, width=.2) +
facet_wrap(~speaker)
sum_corpus_long %>%
ggplot(aes(x=method, y=mean, colour=method)) +
geom_bar() +
geom_errorbar(ymin=sum_corpus_long$min,ymax=sum_corpus_long$max, width=.2) +
facet_wrap(~speaker)
sum_corpus_long %>%
ggplot(aes(x=method, y=mean, colour=method)) +
geom_bar(stat = "identity") +
geom_errorbar(ymin=sum_corpus_long$min,ymax=sum_corpus_long$max, width=.2) +
facet_wrap(~speaker) +
coord_flip() +
xlab("") +
ylab("Mean Complexity") +
theme_minimal() +
ylim(c(0,20))
sum_corpus_long %>%
ggplot(aes(x=speaker, y=mean, colour=speaker)) +
geom_bar(stat = "identity") +
geom_errorbar(ymin=sum_corpus_long$min,ymax=sum_corpus_long$max, width=.2) +
facet_wrap(~method) +
coord_flip() +
xlab("") +
ylab("Mean Complexity") +
theme_minimal() +
ylim(c(0,20))
sum_corpus_long %>%
ggplot(aes(x=speaker, y=mean, colour=speaker)) +
geom_bar(stat = "identity") +
geom_errorbar(ymin=sum_corpus_long$min,ymax=sum_corpus_long$max, width=.2) +
facet_wrap(~method) +
coord_flip() +
xlab("") +
ylab("Mean Complexity") +
theme_minimal()
sum_corpus_long %>%
ggplot(aes(x=speaker, y=mean, fill=speaker)) +
geom_bar(stat = "identity") +
geom_errorbar(ymin=sum_corpus_long$min,ymax=sum_corpus_long$max, width=.2) +
facet_wrap(~method) +
coord_flip() +
xlab("") +
ylab("Mean Complexity") +
theme_minimal()
sum_corpus_long %>%
ggplot(aes(x=speaker, y=mean, fill=speaker)) +
geom_bar(stat = "identity") +
geom_errorbar(ymin=sum_corpus_long$min,ymax=sum_corpus_long$max, width=.2) +
facet_wrap(~method) +
coord_flip() +
xlab("") +
ylab("Mean Complexity") +
guides(fill=FALSE) +
theme_minimal()
speeches$text[1]
library(kableExtra)
library(readr) # more informative and easy way to import data
library(quanteda) # includes functions to implement Lexicoder
library(quanteda.textstats) # for estimating similarity and complexity measures
library(stringdist) # for basic character-based distance measures
library(dplyr) #for wrangling data
library(tibble) #for wrangling data
library(ggplot2) #for visualization
tweets  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/comparison-complexity/cabinet_tweets.rds?raw=true")))
head(tweets)
unique(tweets$username)
length(unique(tweets$username))
#make corpus object, specifying tweet as text field
tweets_corpus <- corpus(tweets, text_field = "tweet")
#add in username document-level information
docvars(tweets_corpus, "username") <- tweets$username
tweets_corpus
dfmat <- dfm(tokens(tweets_corpus, remove_punct = TRUE)) %>%
dfm_remove(pattern = stopwords("english")
dfmat <- dfm(tokens(tweets_corpus, remove_punct = TRUE)) %>%
dfm_remove(pattern = stopwords("english"))
corrmat <- dfmat %>%
dfm_group(groups = username) %>%
textstat_simil(margin = "documents", method = "correlation")
corrmat[1:5,1:5]
#estimate similarity, grouping by username
cos_sim <- dfmat %>%
dfm_group(groups = username) %>%
textstat_simil(margin = "documents", method = "cosine") #specify method here as character object
View(tweets)
View(dfmat)
cosmat <- as.matrix(cos_sim) #convert to a matrix
#generate data frame keeping only the row for Theresa May
cosmatdf <- as.data.frame(cosmat[23, c(1:22, 24)])
#rename column
colnames(cosmatdf) <- "corr_may"
#create column variable from rownames
cosmatdf <- tibble::rownames_to_column(cosmatdf, "username")
ggplot(cosmatdf) +
geom_point(aes(x=reorder(username, -corr_may), y= corr_may)) +
coord_flip() +
xlab("MP username") +
ylab("Cosine similarity score") +
theme_minimal()
#specify different similarity measures to explore
methods <- c("correlation", "cosine", "dice", "edice")
#create empty dataframe
testdf_all <- data.frame()
#gen for loop across methods types
for (i in seq_along(methods)) {
#pass method to character string object
sim_method <- methods[[i]]
#estimate similarity, grouping by username
test <- dfmat %>%
dfm_group(groups = username) %>%
textstat_simil(margin = "documents", method = sim_method) #specify method here as character object created above
testm <- as.matrix(test) #convert to a matrix
#generate data frame keeping only the row for Theresa May
testdf <- as.data.frame(testm[23, c(1:22, 24)])
#rename column
colnames(testdf) <- "corr_may"
#create column variable from rownames
testdf <- tibble::rownames_to_column(testdf, "username")
#record method in new column variable
testdf$method <- sim_method
#bind all together
testdf_all <- rbind(testdf_all, testdf)
}
#create variable (for viz only) that is mean of similarity scores for each MP
testdf_all <- testdf_all %>%
group_by(username) %>%
mutate(mean_sim = mean(corr_may))
ggplot(testdf_all) +
geom_point( aes(x=reorder(username, -mean_sim), y= corr_may, color = method)) +
coord_flip() +
xlab("MP username") +
ylab("Similarity score") +
theme_minimal()
speeches  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/comparison-complexity/speeches.rds?raw=true")))
head(speeches)
speeches$flesch.kincaid <- textstat_readability(speeches$text, measure = "Flesch.Kincaid")
# returned as quanteda data.frame with document-level information;
# need just the score:
speeches$flesch.kincaid <- speeches$flesch.kincaid$Flesch.Kincaid
#get mean and standard deviation of Flesch-Kincaid, and N of speeches for each speaker
sum_corpus <- speeches %>%
group_by(speaker) %>%
summarise(mean = mean(flesch.kincaid, na.rm=TRUE),
SD=sd(flesch.kincaid, na.rm=TRUE),
N=length(speaker))
# calculate standard errors and confidence intervals
sum_corpus$se <- sum_corpus$SD / sqrt(sum_corpus$N)
sum_corpus$min <- sum_corpus$mean - 1.96*sum_corpus$se
sum_corpus$max <- sum_corpus$mean + 1.96*sum_corpus$se
sum_corpus
ggplot(sum_corpus, aes(x=speaker, y=mean)) +
geom_bar(stat="identity") +
geom_errorbar(ymin=sum_corpus$min,ymax=sum_corpus$max, width=.2) +
coord_flip() +
xlab("") +
ylab("Mean Complexity") +
theme_minimal() +
ylim(c(0,20))
